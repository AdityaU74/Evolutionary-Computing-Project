{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810}],"dockerImageVersionId":29869,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyswarms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom keras.callbacks import ReduceLROnPlateau\nimport cv2\nimport os","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pyswarms as ps\nfrom pyswarms.utils.functions import single_obj as fx\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = ['PNEUMONIA', 'NORMAL']\nimg_size = 150\ndef get_training_data(data_dir):\n    data = [] \n    for label in labels: \n        path = os.path.join(data_dir, label)\n        class_num = labels.index(label)\n        for img in os.listdir(path):\n            try:\n                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n                data.append([resized_arr, class_num])\n            except Exception as e:\n                print(e)\n    return np.array(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = get_training_data('../input/chest-xray-pneumonia/chest_xray/chest_xray/train')\ntest = get_training_data('../input/chest-xray-pneumonia/chest_xray/chest_xray/test')\nval = get_training_data('../input/chest-xray-pneumonia/chest_xray/chest_xray/val')","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = []\nfor i in train:\n    if(i[1] == 0):\n        l.append(\"Pneumonia\")\n    else:\n        l.append(\"Normal\")\nsns.set_style('darkgrid')\nsns.countplot(l)        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (5,5))\nplt.imshow(train[0][0], cmap='gray')\nplt.title(labels[train[0][1]])\n\nplt.figure(figsize = (5,5))\nplt.imshow(train[-1][0], cmap='gray')\nplt.title(labels[train[-1][1]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = []\ny_train = []\n\nx_val = []\ny_val = []\n\nx_test = []\ny_test = []\n\nfor feature, label in train:\n    x_train.append(feature)\n    y_train.append(label)\n\nfor feature, label in test:\n    x_test.append(feature)\n    y_test.append(label)\n    \nfor feature, label in val:\n    x_val.append(feature)\n    y_val.append(label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize the data\nx_train = np.array(x_train) / 255\nx_val = np.array(x_val) / 255\nx_test = np.array(x_test) / 255","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# resize data for deep learning \nx_train = x_train.reshape(-1, img_size, img_size, 1)\ny_train = np.array(y_train)\n\nx_val = x_val.reshape(-1, img_size, img_size, 1)\ny_val = np.array(y_val)\n\nx_test = x_test.reshape(-1, img_size, img_size, 1)\ny_test = np.array(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n        featurewise_center=False,  \n        samplewise_center=False,  \n        featurewise_std_normalization=False, \n        samplewise_std_normalization=False,  \n        zca_whitening=False,  \n        rotation_range = 30,\n        zoom_range = 0.2, \n        width_shift_range=0.1,  \n        height_shift_range=0.1,  \n        horizontal_flip = True,  \n        vertical_flip=False) \n\n\ndatagen.fit(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(32 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (150,150,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(256 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Flatten())\nmodel.add(Dense(units = 128 , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units = 1 , activation = 'sigmoid'))\nmodel.compile(optimizer = \"rmsprop\" , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(datagen.flow(x_train,y_train, batch_size = 32) ,epochs = 5 , validation_data = datagen.flow(x_val, y_val) ,callbacks = [learning_rate_reduction])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_model_loss = model.evaluate(x_test,y_test)[0]\noriginal_model_accuracy = model.evaluate(x_test,y_test)[1]*100\n\n# Print results\nprint(\"Loss of the model is - \" , original_model_loss)\nprint(\"Accuracy of the model is - \" , original_model_accuracy , \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  \n        samplewise_center=False,  \n        featurewise_std_normalization=False, \n        samplewise_std_normalization=False, \n        zca_whitening=False,  \n        rotation_range = 30,\n        zoom_range = 0.2,\n        width_shift_range=0.1,  \n        height_shift_range=0.1,\n        horizontal_flip = True,  \n        vertical_flip=False) \n\n\ndatagen.fit(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = [i for i in range(5)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Validation Accuracy')\nax[0].set_title('Training & Validation Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')\nax[1].set_title('Testing Accuracy & Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Training & Validation Loss\")\nplt.show()\n\npredictions = model.predict_classes(x_test)\npredictions = predictions.reshape(1,-1)[0]\npredictions[:15]\n\nprint(classification_report(y_test, predictions, target_names = ['Pneumonia (Class 0)','Normal (Class 1)']))\n\ncm = confusion_matrix(y_test,predictions)\ncm\n\ncm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])\n\nplt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='',xticklabels = labels,yticklabels = labels)\n\ncorrect = np.nonzero(predictions == y_test)[0]\nincorrect = np.nonzero(predictions != y_test)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\nfor c in correct[:6]:\n    plt.subplot(3,2,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(x_test[c].reshape(150,150), cmap=\"gray\", interpolation='none')\n    plt.title(\"Predicted Class {},Actual Class {}\".format(predictions[c], y_test[c]))\n    plt.tight_layout()\n    i += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\nfor c in incorrect[:6]:\n    plt.subplot(3,2,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(x_test[c].reshape(150,150), cmap=\"gray\", interpolation='none')\n    plt.title(\"Predicted Class {},Actual Class {}\".format(predictions[c], y_test[c]))\n    plt.tight_layout()\n    i += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cnn_model(hyperparameters, x_train, y_train, x_val, y_val):\n    learning_rate, num_dense_layers, dropout_rate = hyperparameters\n \n    \n    model = Sequential()\n    model.add(Conv2D(32 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (150,150,1)))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n    model.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n    model.add(Dropout(0.1))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n    model.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n    model.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n    model.add(Conv2D(256 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n    model.add(Flatten())\n    model.add(Dense(units = 128 , activation = 'relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(units = 1 , activation = 'sigmoid'))\n    model.compile(optimizer = \"rmsprop\" , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n    model.summary()\n\n    for _ in range(int(num_dense_layers)):\n        model.add(Dense(units=128, activation='relu'))\n        model.add(Dropout(dropout_rate))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    optimizer = Adam(learning_rate=learning_rate)  \n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    \n    history = model.fit(x_train, y_train, epochs = 12, batch_size=32, validation_data=(x_val, y_val), verbose=0)\n    \n    _, val_accuracy = model.evaluate(x_val, y_val, verbose=0)\n    return val_accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fitness_function(hyperparameters):\n    n_particles = hyperparameters.shape[0]\n    j = []\n    for i in range(n_particles):\n        current_hyperparams = hyperparameters[i]\n        print(f\"Evaluating particle {i+1}/{n_particles} with hyperparameters: {current_hyperparams}\")\n\n        val_accuracy = cnn_model(current_hyperparams, x_train, y_train, x_val, y_val)\n        j.append(-val_accuracy)  \n\n        print(f\"Particle {i+1} evaluation complete. Validation accuracy: {val_accuracy}\")\n\n    return np.array(j)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bounds for hyperparameters: learning rate, number of dense layers, dropout rate, epochs\nbounds = (np.array([0.000001, 1, 0.1]), np.array([0.01, 3, 0.5]))  # Lower and upper bounds\n\noptions = {'c1': 1.49445, 'c2': 1.49445, 'w': 0.729}\n\noptimizer = ps.single.GlobalBestPSO(n_particles=1, dimensions=3, options=options)\n\ncost, pos = optimizer.optimize(fitness_function, iters=1) \n\nbest_learning_rate, best_num_dense_layers, best_dropout_rate = pos\nprint(f'Best Hyperparameters found:\\nlr: {best_learning_rate},\\ndense_layers: {int(best_num_dense_layers)},\\ndropout: {best_dropout_rate}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_hyperparameters = optimizer.pos\nbest_learning_rate, best_num_dense_layers, best_dropout_rate = best_hyperparameters\nbest_num_dense_layers = int(best_num_dense_layers)  # Convert to integer as it represents the number of layers\n\n_, final_model_accuracy, _ = cnn_model([best_learning_rate, best_num_dense_layers, best_dropout_rate], x_train, y_train, x_val, y_val)\n\nprint(f\"Accuracy of the final model with PSO-tuned hyperparameters is: {final_model_accuracy * 100} %\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.bar(['Original', 'Optimized'], [original_model_accuracy, final_model_accuracy])\nplt.ylabel('Accuracy')\nplt.title('Model Performance Comparison')\nplt.ylim([0, 1])  \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}